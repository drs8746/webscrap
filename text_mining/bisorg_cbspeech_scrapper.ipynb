{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download html files from bis.org central banker speech site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('./chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access BIS speech site\n",
    "driver.get(\"https://www.bis.org/cbspeeches/index.htm?m=2%7C10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1601"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_page = driver.find_elements(By.XPATH, \"//div[@class='pageof']\")[0]\n",
    "tot_page = tot_page.find_elements_by_css_selector(\"*\")\n",
    "tot_page = int(tot_page[0].text.split(' ')[-1].replace(',', ''))\n",
    "tot_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "#download folder\n",
    "down_folder = r'..\\data\\bisorg_cbspeech'\n",
    "Path(down_folder).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th page downloaded\n",
      "100th page downloaded\n",
      "200th page downloaded\n",
      "300th page downloaded\n",
      "400th page downloaded\n",
      "500th page downloaded\n",
      "600th page downloaded\n",
      "700th page downloaded\n",
      "800th page downloaded\n",
      "900th page downloaded\n",
      "1000th page downloaded\n",
      "1100th page downloaded\n",
      "1200th page downloaded\n",
      "1300th page downloaded\n",
      "1400th page downloaded\n",
      "1500th page downloaded\n",
      "1600th page downloaded\n"
     ]
    }
   ],
   "source": [
    "for i in range(tot_page):    \n",
    "    _datapage = driver.find_elements(By.XPATH, \"//a[@data-page]\")[-1]\n",
    "    _html = str.encode(driver.page_source)\n",
    "    _filename = 'p{0}.html.txt'.format(10000+i)\n",
    "    with open(os.path.join(down_folder,_filename),'wb') as fout:\n",
    "        fout.write(_html)\n",
    "    _datapage.click()\n",
    "    time.sleep(3)\n",
    "    if i % 100 == 0:\n",
    "        print('{0}th page downloaded'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract link from above html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmlfiles=os.listdir(down_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_speech_list(htmlfile):\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(open(os.path.join(down_folder,htmlfiles[0]), 'rb').read().decode())\n",
    "    content = soup.find(\"table\",{\"class\":\"documentList\"})\n",
    "    \n",
    "    speeches = list()\n",
    "    for i in content.find_all('tr'):\n",
    "        try:\n",
    "            dict_speech = dict()\n",
    "            dict_speech['date'] = i.find(\"td\",{\"class\":\"item_date\"}).text.strip()\n",
    "            dict_speech['link'] = i.find(\"a\")['href'].strip()\n",
    "            dict_speech['title'] = i.find(\"a\").text.strip()\n",
    "            dict_speech['info'] = i.find(\"div\",{\"class\",\"info\"}).text.strip().split('\\n')[0]\n",
    "        except Exception as e:\n",
    "            print('Error({0}): {1}'.format(i,e))\n",
    "            break\n",
    "        speeches.append(dict_speech)\n",
    "    \n",
    "    return speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th files parsed\n",
      "100th files parsed\n",
      "200th files parsed\n",
      "300th files parsed\n",
      "400th files parsed\n",
      "500th files parsed\n",
      "600th files parsed\n",
      "700th files parsed\n",
      "800th files parsed\n",
      "900th files parsed\n",
      "1000th files parsed\n",
      "1100th files parsed\n",
      "1200th files parsed\n",
      "1300th files parsed\n",
      "1400th files parsed\n",
      "1500th files parsed\n",
      "1600th files parsed\n"
     ]
    }
   ],
   "source": [
    "dict_speech = dict()\n",
    "for i, htmlfile in enumerate(htmlfiles):\n",
    "    dict_speech[htmlfile] = generate_speech_list(htmlfile)\n",
    "    if i % 100 == 0:\n",
    "        print('{0}th files parsed'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress and save dict_speech to json \n",
    "import gzip, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.GzipFile('dict_speech.json.gzip','w') as fout:\n",
    "    fout.write(json.dumps(dict_speech).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.GzipFile('dict_speech.json.gzip', 'r') as fin:\n",
    "    dict_speech = json.loads(fin.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download a speech html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bis = r\"https://www.bis.org\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_meta(meta_tags):\n",
    "    dict_meta = dict()\n",
    "    for i in meta_tags:\n",
    "        if i.has_attr('name'):\n",
    "            if i['name'] == 'viewport': #It is not related to Content meta\n",
    "                continue\n",
    "            else:\n",
    "                dict_meta[i['name']] = i['content']\n",
    "    return dict_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _key in dict_speech:\n",
    "    for _speech in dict_speech[_key]:\n",
    "        soup = bs4.BeautifulSoup(requests.get(url_bis+_speech['link']).content.decode()) \n",
    "        _speech['meta'] = extract_meta(soup.find_all(\"meta\"))\n",
    "        _speech['content'] = soup.find(\"div\",{\"id\":\"cmsContent\"}).text.split('\\n')\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
